{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. (wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "In here we are using <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Large Movie Review Dataset</a> from Stanford. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. This dataset provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the dataset\n",
    "def load_dataset(data_dir = \"./data/imdb-reviews/\"):\n",
    "\n",
    "    # Initializing a dictionary for X_data and y_data\n",
    "    X_data = {}\n",
    "    y_data = {}\n",
    "    \n",
    "    # Iterating through the \"train\" and \"test\"\n",
    "    for train_or_test in ['train', 'test']:\n",
    "        \n",
    "        # Initialize an empty dictionary for the train and test\n",
    "        X_data[train_or_test] = {}\n",
    "        y_data[train_or_test] = {}\n",
    "        \n",
    "        # Iterate through \"positive\", \"negative\"\n",
    "        for positive_or_negative in ['positive', 'negative']:\n",
    "            \n",
    "            # Initialize an empty list for each sentiment\n",
    "            X_data[train_or_test][positive_or_negative] = []\n",
    "            y_data[train_or_test][positive_or_negative] = []\n",
    "            \n",
    "            # Get the name of all texts in our folder\n",
    "            file_names = glob.glob(os.path.join(data_dir, train_or_test, positive_or_negative, \"*.txt\")) \n",
    "                \n",
    "            # Iterate through file names\n",
    "            for i_file in file_names:\n",
    "                \n",
    "                # Open the (text) file\n",
    "                with open(i_file) as i_file:\n",
    "                \n",
    "                    # Assign values to our dictionary from that file\n",
    "                    X_data[train_or_test][positive_or_negative].append(i_file.read())\n",
    "                    y_data[train_or_test][positive_or_negative].append(positive_or_negative)\n",
    "                \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_data, y_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " 12500 Positive / 12500 Negative\n",
      "\n",
      "Test set:\n",
      " 12500 Positive / 12500 Negative\n"
     ]
    }
   ],
   "source": [
    "# Get the shape dataset\n",
    "print(\"Training set:\\n {} Positive / {} Negative\\n\".format(len(X_data[\"train\"][\"positive\"]), \n",
    "                                                           len(X_data[\"train\"][\"negative\"])))\n",
    "\n",
    "print(\"Test set:\\n {} Positive / {} Negative\".format(len(X_data[\"test\"][\"positive\"]), \n",
    "                                                   len(X_data[\"test\"][\"negative\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test set\n",
    "X_train = X_data[\"train\"][\"positive\"] + X_data[\"train\"][\"negative\"]\n",
    "y_train = y_data[\"train\"][\"positive\"] + y_data[\"train\"][\"negative\"]\n",
    "\n",
    "X_test = X_data[\"test\"][\"positive\"] + X_data[\"test\"][\"negative\"]\n",
    "y_test = y_data[\"test\"][\"positive\"] + y_data[\"test\"][\"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffling the trianing set and test ste\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set = 25000 \n",
      "Test set = 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set = {} \\nTest set = {}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a small subset of dataset (for speed purposes)\n",
    "X_train, y_train = X_train[:4000], y_train[:4000]\n",
    "X_test, y_test = X_test[:1000], y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "At the second step, We will prerpocess our dataset which is an essential part of any type of model. More specifically we will apply the following steps for preprocessing:\n",
    "1. Lowercasing the text\n",
    "2. Removing the punctuation\n",
    "3. Converting to tokens\n",
    "4. Removing the stopwords\n",
    "5. Apply stemmer\n",
    "6. Apply lemmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from keras.preprocessing import sequence\n",
    "import bs4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Removing all HTML tags\n",
    "    text = bs4.BeautifulSoup(text, \"html5lib\").get_text().strip()\n",
    "    \n",
    "    # Lowercasing the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # Converting to tokens\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing the stopwords\n",
    "    tokens = [i_token for i_token in tokens if i_token not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Apply stemmer\n",
    "    stemmed = [PorterStemmer().stem(i_token) for i_token in tokens]\n",
    "\n",
    "    # Apply lemmizer\n",
    "    lemmtized = [WordNetLemmatizer().lemmatize(i_token, pos=\"n\") for i_token in stemmed]\n",
    "    lemmtized = [WordNetLemmatizer().lemmatize(i_token, pos=\"v\") for i_token in lemmtized]\n",
    "\n",
    "    return lemmtized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproces the training set and test set\n",
    "X_train = [preprocess_text(i) for i in X_train]\n",
    "X_test = [preprocess_text(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total dataset\n",
    "total_dataset = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2id and id2word\n",
    "all_unique_words = np.unique([item for sub_list in total_dataset for item in sub_list])\n",
    "word2id = {i_token: index for index, i_token in enumerate(all_unique_words)}\n",
    "id2word = {index: i_token for index, i_token in enumerate(all_unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping words in training to its corresponding id\n",
    "for index, sub_list in enumerate(X_train):\n",
    "    X_train[index] = list(map(lambda x: word2id[x], sub_list))\n",
    "    \n",
    "# Mapping words in test set to its corresponding id\n",
    "for index, sub_list in enumerate(X_test):\n",
    "    X_test[index] = list(map(lambda x: word2id[x], sub_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequence\n",
    "max_words = 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert labels into 0 and 1\n",
    "def str_to_int(label):\n",
    "    if label == \"positive\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "y_train = list(map(str_to_int, y_train))\n",
    "y_test = list(map(str_to_int, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Now we are ready to feed the data into our model for training. As you will see, Even with a simple architecture we can reach to a high accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Reshape\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters\n",
    "embedding_size = 32\n",
    "lstm_units = 100\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "vocabulary_size = len(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           831360    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 884,661\n",
      "Trainable params: 884,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length = max_words))\n",
    "model.add(LSTM(units = lstm_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 119s 30ms/step - loss: 0.6959 - acc: 0.5410 - val_loss: 0.6726 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67256, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 109s 27ms/step - loss: 0.6322 - acc: 0.7300 - val_loss: 0.5844 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67256 to 0.58436, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 121s 30ms/step - loss: 0.3782 - acc: 0.8752 - val_loss: 0.3958 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58436 to 0.39578, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 119s 30ms/step - loss: 0.1643 - acc: 0.9470 - val_loss: 0.3696 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39578 to 0.36961, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 120s 30ms/step - loss: 0.0735 - acc: 0.9795 - val_loss: 0.3954 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36961\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 120s 30ms/step - loss: 0.0342 - acc: 0.9925 - val_loss: 0.4386 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36961\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 120s 30ms/step - loss: 0.0226 - acc: 0.9948 - val_loss: 0.5179 - val_acc: 0.8460\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.36961\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 108s 27ms/step - loss: 0.0130 - acc: 0.9982 - val_loss: 0.6540 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36961\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 108s 27ms/step - loss: 0.0059 - acc: 0.9990 - val_loss: 0.6062 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36961\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.6709 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5ca71dd8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint for saving the model\n",
    "checkpointer = ModelCheckpoint(filepath='./saved model/weights.best.sentiment_analysis.hdf5', \n",
    "                               verbose = 1, \n",
    "                               save_best_only = True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          batch_size = batch_size,\n",
    "          epochs = num_epochs,\n",
    "          callbacks = [checkpointer], \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Once you have trained your model, it's time to see how well it performs on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 83.3%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test Set Accuracy: {}%\".format(scores[1]*100))  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction\n",
    "\n",
    "Now you are ready for prediction. You can check the sentiment of any sentence you input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for prediction\n",
    "def text_to_predict(unseen_text):\n",
    "    \n",
    "    # Preprocess the text\n",
    "    unseen_text = preprocess_text(unseen_text)\n",
    "    \n",
    "    # Convert the words to ids\n",
    "    unseen_text = list(map(lambda x: word2id[x], unseen_text))\n",
    "    \n",
    "    # Pad sequences\n",
    "    unseen_text = sequence.pad_sequences([unseen_text], max_words)\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = model.predict(unseen_text)[0][0]*100\n",
    "\n",
    "    # Print the result\n",
    "    if prediction < 0.6:\n",
    "        print(\"The given sentence is negative.\")\n",
    "    elif prediction > 0.6:\n",
    "        print(\"The given sentence is positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sentence is negative.\n"
     ]
    }
   ],
   "source": [
    "# Predict a unseen text\n",
    "unseen_text = \"The movie is absolutely terrible. It's not something i would suggest\"\n",
    "text_to_predict(unseen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sentence is positive.\n"
     ]
    }
   ],
   "source": [
    "# Predict a unseen text\n",
    "unseen_text = \"The movie is absolutely great. Can't wait to watch it again.\"\n",
    "text_to_predict(unseen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
